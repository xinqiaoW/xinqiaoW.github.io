---
toc: true
tags: RL AI
---
## MuZero 整体架构
MuZero 是一种强化学习算法，在大名鼎鼎的 AlphaZero 的基础上有一定的突破与改进。MuZero 主要有预测三个函数：     
我们首先介绍一些符号，$o^k$ 代表第 k 步观察到的状态，$r^k$ 代表预测第 k 步获得的即时奖励，$s^k$ 代表第 k 步的隐藏状态，$a^k$ 代表第 k 步执行的动作，$u_k$ 代表
第 k 步观察到的即时奖励，$\gamma$ 代表折扣因子，$\textbf{p}_t^k$、$v_t^k$ 分别代表观察状态以 $(o_1, o_2, \dots, o_t)$ 为初始，进行 k 步推演（或者说，玩游戏）时模型预测的策略以及价值。
- 表示函数h: 用于将观测到的状态空间转换为 **Hidden State**;
- 动态函数g: 用于根据现在的隐藏状态 $s^{k - 1}$ 和该状态下执行的动作 $a^k$ 预测即时奖励，以及下一个隐藏状态: $$r^k, s^k=g(s^{k-1}, a^k)$$
- 预测函数f: 用于预测价值 $\mathcal{E}[u_{t + k + 1} + \gamma u_{t + k + 2} + \gamma^2 u_{t + k + 3}+ \dots|o_1, o_2, \dots, o_t, a_{t+1}, a_{t+2}, \dots, a_{t+k}]$，以及
以及策略 $\pi(a_{t + k + 1}|o_1, o_2, \dots, o_t, a_{t+1}, a_{t+2}, \dots, a_{t+k})$。即 $\textbf{p}_t^k$, $v_t^k$ = $f(s^k)$。

## MuZero 流程
假设观察状态以 $(o_1, o_2, \dots, o_t)$ 为初始，首先通过表示函数将其转换为隐藏状态 $s_0=h(o_1, o_2, \dots, o_t)$，我们进行 MCTS 搜索（或者其他搜索方案），得到每一个步骤（Step k 从 0  到 K） $\pi(a_{t + k + 1}|o_1, o_2, \dots, o_t, a_{t+1}, a_{t+2}, \dots, a_{t+k})$、
$\mathcal{E}[u_{t + k + 1} + \gamma u_{t + k + 2} + \gamma^2 u_{t + k + 3}+ \dots|o_1, o_2, \dots, o_t, a_{t+1}, a_{t+2}, \dots, a_{t+k}]$ 以及 即时奖励 $u_{t+k}$。
由于这些是展望多个步骤（游戏模拟了很多路径）得到的值，相对来说比较准确。我们希望我们的预测值能够更好地预测这些数值。因此以这些数值为监督信号，构建联合损失函数并反向传播。

## 联合损失函数
$l_t(\theta) = \sum_{k=0}^{K} l^r(u_{t+k}, r_t^k) + l^v(z_{t+k}, v_t^k) + l^p(\pi_{t+k}, \mathbf{p}_t^k) + c\parallel\theta\parallel^2$
上面即为损失函数，为了能够同时训练所有预测函数，我们将目标融合在一起，我们有三个目标
- 使得 $u_{t+k}$ 以及 $r_t^k$ 接近，即预测的即时奖励和观察到的即时奖励接近；
- 使得 $\pi_{t+k}$ 以及 $\mathbf{p}_t^k$ 接近，即预测的策略和 MCTS 搜索得出的策略接近；
- 使得 $z_{t+k}=u_{t+k+1} + \gamma u_{t+k+2} + \gamma^2 u_{t+k+3} + \dots + \gamma^{n-1} u_{t +k+n} + \gamma^{n} v_{t+n}$ 与 $v_t^k$ 接近。即从 k 步开始，进行长达 n 步的采样，中间会获得 n 步的即时奖励
，将这 n 步的即时奖励和 n 步后的预测价值结合，作为我们的预测值 $v_t^k$ 需要接近的目标。
这里我们使用了预测价值 $v_{t+n}$ 作为真实价值，最初的时候可能这个值并不准确，但是由于我们进行了 n 步展开采样搜索，所以比纯粹的使用预测值要稍微准确一点，我们又会让预测值接近这个值，进而使得预测的价值稍微准确一点。不停地训练，使得价值越来越准确。**这是与 AlphaZero 不同的，AlphaZero 是没有即时奖励的，无法使用这种方式，AlphaZero 仅仅使用游戏结局作为监督信号，或者说奖励信号。**
