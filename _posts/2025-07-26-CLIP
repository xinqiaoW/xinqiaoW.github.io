CLIP 提出在超大规模数据集上，用文本监督信号，通过对比学习来训练一个超大模型，能够很好地提取文本和图像特征，并形成文本特征和图像特征的对应，为多模态的工作做了铺垫。

混精度训练：
	是一种可以大幅加速模型训练，减小内存占用，同时也不影响模型精度的技术。

CLIP的架构：
收集大量的 （text，img）的pair 将 img 输入到 img-encoder 中，将 text 输入到 text-encoder 中，得到图片文本特征，我们将 同一对 中的 图片特征和文本特征作为正样本，其他作为负样本进行对比学习。相当于用一个更复杂的带有上下文语义信息的文本去做监督信号，使得模型的能力提升。

![[CLIP.png]]
如上图左半部分，我们希望对比学习使得 同一对 中的 图片特征和文本特征更接近（内积越大），不同对的 图片特征和文本特征相互远离。

理想结果是上图左半部分矩阵的值集中在蓝色对角线方块，其余值很小。

上图左半部分展示了如何进行分类，给定一系列文本以及其对应特征。我们将图片特征与文本特征进行比对，找到与图片特征最接近的文本特征。对应的文本就可以对图片进行分类。

其优势在于：
	1 - 可以用更大的数据，减少了模型的标注成本。
	 不再需要严格的标注数据，可以利用海量的互联网数据去训练模型。
	2 - 提供了更丰富的语义信息，使得模型可以理解一定的情景（上下文）。
	例如，模型可以在大量数据喂养后，可能会将‘红包’，‘中国’，‘新年’联系起来，模型具备了一定的情景关联的能力，可以更好地理解一些多义词。
	3 - 为多模态工作打了基础
	模型可以将文本和图片关联起来，为多模态工作做了铺垫。
	4 - 在各种下游任务中效果都很好。

不足：
	1 - 数据集来源于网络，有有害数据的污染
	2 - 模型的参数量，数据量很大，训练起来很慢
	3 - 无法理解太过抽象的概念
	4 - 图片与数据集的分布实在不一样的时候，模型也很难分辨

总的来说，CLIP 的模型效果很好，甚至能在 zero-shot 上取得较好结果，也为多模态领域打下了基础，但它也有着不足， 仍然需要改进。
